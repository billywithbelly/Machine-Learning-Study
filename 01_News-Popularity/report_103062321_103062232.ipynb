{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team ML makes me cry\n",
    "\n",
    "Team member and contribution :\n",
    "* 陳博安 103062321 : 資料處理 40%\n",
    "* 谷佳駿 103062232 : trainning 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before preprocessing, what we had was just a lot of text data, which was hard for us to use to train our model, so we needed to use HashingVectorizer to convert our data into vectors.(We've tried the tfidfVectorizer, but it requires more momory than we have, so we didn't use it.)\n",
    "\n",
    "We found news titles might be a factor that affects people's decision as to whether to read the content of news, which may be related to its popularity, so we decided to single out titles and vectorize them as features to train our model.\n",
    "We also found which day of the week the news came out may help us with our predictions, so we added it as features after one-hot encoding it.\n",
    "\n",
    "Before vectorizeing text data, we first tokenized it into lists of strings, and then stemmed them and filtered out the stop words in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/billywithbelly/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "def get_processed_data(X):\n",
    "    content = []\n",
    "    titles = []\n",
    "    weekdays = []\n",
    "    for i in range(X.shape[0]):\n",
    "        text = X[i]\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        date = soup.find('time')\n",
    "        title = soup.find('h1')\n",
    "        if date.string != None:\n",
    "            date = parser.parse(date.string).weekday()\n",
    "        else:\n",
    "            date = 0\n",
    "        title = re.sub('[\\W]+', ' ', title.string.lower())\n",
    "        \n",
    "        content.append(text)\n",
    "        titles.append(title)\n",
    "        weekdays.append(date)\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse=False, n_values=7)\n",
    "    weekdays = np.array(weekdays)\n",
    "    one_hot_weekdays = ohe.fit_transform(weekdays[:, np.newaxis])\n",
    "   \n",
    "    #tfidf = TfidfVectorizer(ngram_range=(1,1), tokenizer=tokenizer_stem_nostop)\n",
    "    hashVec = HashingVectorizer(n_features=2**14, tokenizer=tokenizer_stem_nostop)\n",
    "    content = np.array(content).reshape(-1, 1)\n",
    "    titles = np.array(titles).reshape(-1, 1)\n",
    "    content_tfidf = hashVec.transform(content.flatten()).toarray()\n",
    "    titles_tfidf = hashVec.transform(titles.flatten()).toarray()\n",
    "    \n",
    "    \n",
    "    return np.concatenate((titles_tfidf, content_tfidf, one_hot_weekdays), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing,  we were finally able to train our model.\n",
    "We used the SGDClassifier model since our computers were not powerful enough to allow us to trian all the data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a06da0d970c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Page content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Popularity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import _pickle as pkl\n",
    "\n",
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', n_iter=100)\n",
    "batch_size =200 \n",
    "stream = get_stream(path='datasets/train.csv', size=batch_size)\n",
    "classes = np.array([-1, 1])\n",
    "train_auc, val_auc = [], []\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((40000+batch_size-1)/(batch_size))\n",
    "\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['Page content'], batch['Popularity']\n",
    "    if X_train is None:\n",
    "        break\n",
    "    processed_X_train = get_processed_data(X_train)\n",
    "    clf.partial_fit(processed_X_train, y_train, classes=classes)\n",
    "    train_auc.append(roc_auc_score(y_train, clf.predict_proba(processed_X_train)[:,1]))\n",
    "    \n",
    "pkl.dump(clf, open('output/clf-sgd-final.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the model trained, we used the model to predict the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/25000]\n",
      "[200/25000]\n",
      "[300/25000]\n",
      "[400/25000]\n",
      "[500/25000]\n",
      "[600/25000]\n",
      "[700/25000]\n",
      "[800/25000]\n",
      "[900/25000]\n",
      "[1000/25000]\n",
      "[1100/25000]\n",
      "[1200/25000]\n",
      "[1300/25000]\n",
      "[1400/25000]\n",
      "[1500/25000]\n",
      "[1600/25000]\n",
      "[1700/25000]\n",
      "[1800/25000]\n",
      "[1900/25000]\n",
      "[2000/25000]\n",
      "[2100/25000]\n",
      "[2200/25000]\n",
      "[2300/25000]\n",
      "[2400/25000]\n",
      "[2500/25000]\n",
      "[2600/25000]\n",
      "[2700/25000]\n",
      "[2800/25000]\n",
      "[2900/25000]\n",
      "[3000/25000]\n",
      "[3100/25000]\n",
      "[3200/25000]\n",
      "[3300/25000]\n",
      "[3400/25000]\n",
      "[3500/25000]\n",
      "[3600/25000]\n",
      "[3700/25000]\n",
      "[3800/25000]\n",
      "[3900/25000]\n",
      "[4000/25000]\n",
      "[4100/25000]\n",
      "[4200/25000]\n",
      "[4300/25000]\n",
      "[4400/25000]\n",
      "[4500/25000]\n",
      "[4600/25000]\n",
      "[4700/25000]\n",
      "[4800/25000]\n",
      "[4900/25000]\n",
      "[5000/25000]\n",
      "[5100/25000]\n",
      "[5200/25000]\n",
      "[5300/25000]\n",
      "[5400/25000]\n",
      "[5500/25000]\n",
      "[5600/25000]\n",
      "[5700/25000]\n",
      "[5800/25000]\n",
      "[5900/25000]\n",
      "[6000/25000]\n",
      "[6100/25000]\n",
      "[6200/25000]\n",
      "[6300/25000]\n",
      "[6400/25000]\n",
      "[6500/25000]\n",
      "[6600/25000]\n",
      "[6700/25000]\n",
      "[6800/25000]\n",
      "[6900/25000]\n",
      "[7000/25000]\n",
      "[7100/25000]\n",
      "[7200/25000]\n",
      "[7300/25000]\n",
      "[7400/25000]\n",
      "[7500/25000]\n",
      "[7600/25000]\n",
      "[7700/25000]\n",
      "[7800/25000]\n",
      "[7900/25000]\n",
      "[8000/25000]\n",
      "[8100/25000]\n",
      "[8200/25000]\n",
      "[8300/25000]\n",
      "[8400/25000]\n",
      "[8500/25000]\n",
      "[8600/25000]\n",
      "[8700/25000]\n",
      "[8800/25000]\n",
      "[8900/25000]\n",
      "[9000/25000]\n",
      "[9100/25000]\n",
      "[9200/25000]\n",
      "[9300/25000]\n",
      "[9400/25000]\n",
      "[9500/25000]\n",
      "[9600/25000]\n",
      "[9700/25000]\n",
      "[9800/25000]\n",
      "[9900/25000]\n",
      "[10000/25000]\n",
      "[10100/25000]\n",
      "[10200/25000]\n",
      "[10300/25000]\n",
      "[10400/25000]\n",
      "[10500/25000]\n",
      "[10600/25000]\n",
      "[10700/25000]\n",
      "[10800/25000]\n",
      "[10900/25000]\n",
      "[11000/25000]\n",
      "[11100/25000]\n",
      "[11200/25000]\n",
      "[11300/25000]\n",
      "[11400/25000]\n",
      "[11500/25000]\n",
      "[11600/25000]\n",
      "[11700/25000]\n",
      "[11800/25000]\n",
      "[11900/25000]\n",
      "[12000/25000]\n",
      "[12100/25000]\n",
      "[12200/25000]\n",
      "[12300/25000]\n",
      "[12400/25000]\n",
      "[12500/25000]\n",
      "[12600/25000]\n",
      "[12700/25000]\n",
      "[12800/25000]\n",
      "[12900/25000]\n",
      "[13000/25000]\n",
      "[13100/25000]\n",
      "[13200/25000]\n",
      "[13300/25000]\n",
      "[13400/25000]\n",
      "[13500/25000]\n",
      "[13600/25000]\n",
      "[13700/25000]\n",
      "[13800/25000]\n",
      "[13900/25000]\n",
      "[14000/25000]\n",
      "[14100/25000]\n",
      "[14200/25000]\n",
      "[14300/25000]\n",
      "[14400/25000]\n",
      "[14500/25000]\n",
      "[14600/25000]\n",
      "[14700/25000]\n",
      "[14800/25000]\n",
      "[14900/25000]\n",
      "[15000/25000]\n",
      "[15100/25000]\n",
      "[15200/25000]\n",
      "[15300/25000]\n",
      "[15400/25000]\n",
      "[15500/25000]\n",
      "[15600/25000]\n",
      "[15700/25000]\n",
      "[15800/25000]\n",
      "[15900/25000]\n",
      "[16000/25000]\n",
      "[16100/25000]\n",
      "[16200/25000]\n",
      "[16300/25000]\n",
      "[16400/25000]\n",
      "[16500/25000]\n",
      "[16600/25000]\n",
      "[16700/25000]\n",
      "[16800/25000]\n",
      "[16900/25000]\n",
      "[17000/25000]\n",
      "[17100/25000]\n",
      "[17200/25000]\n",
      "[17300/25000]\n",
      "[17400/25000]\n",
      "[17500/25000]\n",
      "[17600/25000]\n",
      "[17700/25000]\n",
      "[17800/25000]\n",
      "[17900/25000]\n",
      "[18000/25000]\n",
      "[18100/25000]\n",
      "[18200/25000]\n",
      "[18300/25000]\n",
      "[18400/25000]\n",
      "[18500/25000]\n",
      "[18600/25000]\n",
      "[18700/25000]\n",
      "[18800/25000]\n",
      "[18900/25000]\n",
      "[19000/25000]\n",
      "[19100/25000]\n",
      "[19200/25000]\n",
      "[19300/25000]\n",
      "[19400/25000]\n",
      "[19500/25000]\n",
      "[19600/25000]\n",
      "[19700/25000]\n",
      "[19800/25000]\n",
      "[19900/25000]\n",
      "[20000/25000]\n",
      "[20100/25000]\n",
      "[20200/25000]\n",
      "[20300/25000]\n",
      "[20400/25000]\n",
      "[20500/25000]\n",
      "[20600/25000]\n",
      "[20700/25000]\n",
      "[20800/25000]\n",
      "[20900/25000]\n",
      "[21000/25000]\n",
      "[21100/25000]\n",
      "[21200/25000]\n",
      "[21300/25000]\n",
      "[21400/25000]\n",
      "[21500/25000]\n",
      "[21600/25000]\n",
      "[21700/25000]\n",
      "[21800/25000]\n",
      "[21900/25000]\n",
      "[22000/25000]\n",
      "[22100/25000]\n",
      "[22200/25000]\n",
      "[22300/25000]\n",
      "[22400/25000]\n",
      "[22500/25000]\n",
      "[22600/25000]\n",
      "[22700/25000]\n",
      "[22800/25000]\n",
      "[22900/25000]\n",
      "[23000/25000]\n",
      "[23100/25000]\n",
      "[23200/25000]\n",
      "[23300/25000]\n",
      "[23400/25000]\n",
      "[23500/25000]\n",
      "[23600/25000]\n",
      "[23700/25000]\n",
      "[23800/25000]\n",
      "[23900/25000]\n",
      "[24000/25000]\n",
      "[24100/25000]\n",
      "[24200/25000]\n",
      "[24300/25000]\n",
      "[24400/25000]\n",
      "[24500/25000]\n",
      "[24600/25000]\n",
      "[24700/25000]\n",
      "[24800/25000]\n",
      "[24900/25000]\n",
      "[25000/25000]\n",
      "[25100/25000]\n",
      "[25200/25000]\n",
      "[25300/25000]\n",
      "[25400/25000]\n",
      "[25500/25000]\n",
      "[25600/25000]\n",
      "[25700/25000]\n",
      "[25800/25000]\n",
      "[25900/25000]\n",
      "[26000/25000]\n",
      "[26100/25000]\n",
      "[26200/25000]\n",
      "[26300/25000]\n",
      "[26400/25000]\n",
      "[26500/25000]\n",
      "[26600/25000]\n",
      "[26700/25000]\n",
      "[26800/25000]\n",
      "[26900/25000]\n",
      "[27000/25000]\n",
      "[27100/25000]\n",
      "[27200/25000]\n",
      "[27300/25000]\n",
      "[27400/25000]\n",
      "[27500/25000]\n",
      "[27600/25000]\n",
      "[27700/25000]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dfbaa96fc6c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Page content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "# loss='log' gives logistic regression\n",
    "#modify here\n",
    "clf = pkl.load(open('output/clf-sgd-final.pkl', 'rb'))\n",
    "batch_size =100 \n",
    "stream = get_stream(path='datasets/train.csv', size=batch_size)\n",
    "#modify here\n",
    "y_train_B_predict = np.array([])\n",
    "\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((30000+batch_size)/(batch_size))\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_test = batch['Page content']\n",
    "    if X_test is None:\n",
    "        break\n",
    "    processed_X_test = get_processed_data(X_test)\n",
    "    #modify here\n",
    "    y_train_B_predict = np.concatenate( (y_train_B_predict, clf.predict(processed_X_test)), axis = 0 )\n",
    "    print('[{}/{}]'.format((i+1)*(batch_size), 25000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-core learning:\n",
    "Since we didn't have enough time, we've only tried out SGDClassifier and LogisticRegression. The latter didn't work ㄋsince we have not enough memory, which tells us the improtance of out-of-core leaning and that The models we can use are limited by the computers' performance.\n",
    "\n",
    "Consistency of features:\n",
    "When using out-of-core learning models, we encountered a pitfall where when we tried to one-hot-encode the weekday feature of a small part of the training dataset, since the part of the dataset didn't contain all seven days, the one-hot encoder only produced 6 columns, which led to inconsistency of number of columns. However it wasn't hard so solve the problem. All we did to solve the problem was set the n_values argument of OneHotEncoder to 7. From this expirience, we learned that we need to make sure the data features of each set of data after preprocessing are consistent when using Out-of-core algorithms.\n",
    "\n",
    "Preprocessing:\n",
    "In real life, a lot of data is not numerical and needs to be preprocessed before being applied to models, just as we did in the compitition. Before we preprocessed the data, it couldn't be applied to a model. Moreover, How well a model predicts could mostly depend on how well the data is preprocessed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
