{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/billywithbelly/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 14 19:05:37 2016\n",
    "\n",
    "@author: James\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import _pickle as pkl\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/train.csv')\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    date = soup.find('time')\n",
    "    title = soup.find('h1')\n",
    "    date = parser.parse(date.string).weekday()\n",
    "    title = re.sub('[\\W]+', ' ', title.string.lower())\n",
    "          \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    return text, title, date\n",
    "    \n",
    "content, titles, weekdays = zip(*[preprocessor(df['Page content'].iloc[i]) for i in range(df.shape[0])])\n",
    "content = np.array(content).reshape(-1, 1)\n",
    "titles = np.array(titles).reshape(-1, 1)\n",
    "weekdays = np.array(weekdays).reshape(-1, 1)\n",
    "y = df['Popularity']\n",
    " \n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "one_hot_weekdays = ohe.fit_transform(weekdays)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1), tokenizer=tokenizer_stem_nostop)\n",
    "content_tfidf = tfidf.fit_transform(content.flatten()).toarray()\n",
    "titles_tfidt = tfidf.fit_transform(titles.flatten()).toarray()\n",
    "combined_X = np.concatenate((content_tfidf, titles_tfidt, one_hot_weekdays), axis = 1)\n",
    "\n",
    " \n",
    "#pipe = Pipeline( [ ('vectorize', TfidfVectorizer(tokenizer = tokenizer_stem_nostop) ), ('classifier', LogisticRegression) ] )\n",
    "train_score = cross_val_score(estimator = LogisticRegression(), X = combined_X, y = y, cv = 5, scoring = 'roc_auc')\n",
    "logistic = LogisticRegression()\n",
    "logistic.fit(combined_X, y)\n",
    "#pkl.dump(logistic, open('output/clf-sgd.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
