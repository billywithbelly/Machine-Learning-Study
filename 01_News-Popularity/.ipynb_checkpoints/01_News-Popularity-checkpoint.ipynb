{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "72e3ced7-47d5-4dd8-a8fd-16bf557832e0"
    }
   },
   "source": [
    "# <center>DataLab Cup 1: Predicting News Popularity</center>\n",
    "\n",
    "<center>Shan-Hung Wu &amp; DataLab<br/>Fall 2016</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5fd5d108-fe33-4ba0-93ed-3db1d3cbf537"
    }
   },
   "source": [
    "## Competition Info\n",
    "\n",
    "In this competition, you are provided with a supervised dataset $\\mathbb{X}$ consisting of the **raw content** of news articles and the binary **popularity** (where $1$ means \"popular\" and $-1$ not, calculated based on the number of shares in online social networking services) of these articles as labels. Your goal is to learn a function $f$ from $\\mathbb{X}$ that is able to predict the popularity of an unseen news article.\n",
    "\n",
    "#### Dataset Format\n",
    "\n",
    "* `train.csv` contains 27643 data points (news articles) with attributes `Id`, `Page content`, and binary labels `Popularity`\n",
    "* `test.csv` contains 11848 data points with the only the attributes.\n",
    "\n",
    "#### How to Submit Results?\n",
    "\n",
    "You have to predict the correct labels of data points in `test.csv` and submit your predictions to the [Kaggle-In-class](https://inclass.kaggle.com/c/datalabcup-2016-news-popularity) online judge system to get scores. Following are some example actions:\n",
    "\n",
    "<table>\n",
    "<tr><td>Panel</td><td>Action</td><td>Description</td></tr>\n",
    "<tr><th rowspan=\"7\"><img src=\"https://drive.google.com/a/datalab.cs.nthu.edu.tw/uc?id=0BxGBu16r86Q0cF95TndSaGExSFE\" width=\"500\"></th>\n",
    "<td>Data</td><td>Get the dataset $\\mathbb{X}$ here.</td></tr>\n",
    "<tr><td>Make a Submission</td><td>Your testing performance will be evaluated immediately and shown on the leaderboard.</td></tr>\n",
    "<tr><td>Leaderboard</td><td>The current ranking of participants. Note that this ranking only reflects the performance on part of the testset and may not equal to the final ranking (see below).</td></tr>\n",
    "<tr><td>Forum</td><td>You can ask questions or share findings here.</td></tr>\n",
    "<tr><td>Kernels</td><td>You can create your jupyter notebook, run it, and keep it as private or public here.</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "#### Scoring\n",
    " \n",
    "The evaluation metric is AUC. The ranking shown on the leaderboard before the end of competition reflects only the AUC performance over **part of** `test.csv`. However, this is **not** how we evaluate your final scores. After the competition, we calculate AUC over the entire `test.csv` and report the final ranking thereby.\n",
    "\n",
    "There will be two baseline results, namely, `Benchmark-60` and `Benchmark-80`. You have to outperform `Benchmark-60` to get 60 points, and `Benchmark-80` to get 80. Meanwhile, the higher AUC you achieve, the higher the final score you will get. \n",
    "\n",
    "#### Rules (Subject to Change)\n",
    "\n",
    "What you can do:\n",
    "\n",
    "* Use untaught APIs: you can use any machine learning tools you like as well as models/techniques that are not taught in the class.\n",
    " \n",
    "What you **can't** do:\n",
    "\n",
    "* Attempt to make predictions by means other than \"learning\" from the given dataset $\\mathbb{X}$ or related sources. For example, you cannot hard code decision rules in your submission, but you can use a `DecisionTreeClassifier` that learns rules from data.\n",
    "\n",
    "Violation of any prohibited rule will be considered as cheating and results in **0 final score**.\n",
    "\n",
    "#### Important Dates\n",
    "\n",
    "* 11/1/2016 (Tue) - competition starts;\n",
    "* 11/15/2016 (Tue) 23:59pm - competition ends, final score announcement;\n",
    "* 11/16/2016 (Wed) 23:59pm - report submission (to iLMS);\n",
    "* 11/17/2016 (Thu) - winner presentation (top-3 teams, 15 min each).\n",
    "\n",
    "\n",
    "#### Competition Report\n",
    "\n",
    "After the competition, **each team have to hand in a report** in Jupyter notebook format via the iLMS system. You report should include:\n",
    "\n",
    "* Student ID, name of each team member\n",
    "* How did you preprocess data (cleaning, feature engineering, etc)?\n",
    "* How did you build the classifier (model, training algorithm, special techniques,etc.)?\n",
    "* Conclusions (interesting findings, pitfalls, takeaway lessons, etc.)?\n",
    " \n",
    "The file name of your report must be: `LSML-{Your Team number}-report.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f87c575a-3d03-4fa3-86e9-f3f61cd8d12e"
    }
   },
   "source": [
    "## Hint 1: Feature Engineering is More Important Then You Expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "022c5a4c-be2a-446e-8bb8-b96840e26ef4"
    }
   },
   "source": [
    "So far, we learn various machine learning techniques based on datasets where the date features are predefined. In many real-world applications, including this competition, we only get raw data and have to define the features ourself. **Feature engineering** is the process of using domain knowledge to create features that make machine learning algorithms work. While good modeling and training techniques help you make better predictions, feature engineering usually determines whether your task is \"learnable\". \n",
    "\n",
    "To demonstrate the importance of feature engineering, let's use the [IMDB review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) to show how to extract meaningful information from a subset of these movie reviews to build a machine learning model that can predict whether a certain reviewer liked or disliked a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "24e196f7-3445-4a7c-83a0-d473bf0424fc"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  I know that Chill Wills usually played lovable...          1\n",
      "1  The arrival of an world famous conductor sets ...          1\n",
      "2  This documentary is such a wonderful example o...          1\n",
      "3  I really tried to like this movie but in the e...          0\n",
      "4  Not one of Monogram's better(not trying to be ...          0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/train.csv')\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fe525aa5-2bf8-49f8-b8f0-a0f9ded98339"
    }
   },
   "source": [
    "We get movie reviews in its raw content---there is only one feature called `review`. If the review is positive comment, then the label field `sentiment` equals to 1; otherwise 0. To be able to predict from text, we have to go through several preprocessing steps first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f9573516-aec6-4362-ae0c-df571e505c2b"
    }
   },
   "source": [
    "#### Preprocessing: Data Cleaning\n",
    "\n",
    "Data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate pieces of information in the dataset. Let's print a review and see if we need to clean up the raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b89adb99-9c84-4ee1-9949-38a8d10e2a8e"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know that Chill Wills usually played lovable old sorts in Westerns. But his role in this segment is something I've remembered for a long time. Wills could be a first rate villain. Yes, Burgess Meredith's Fall was correct! That look in Hepplewhite's eye! It expressed porcine greed, ignorance, and the threat of violence all at once. Quite a performance, I think.<br /><br />The segment itself was a good one, too. Question: couldn't the little black bag cure alcoholism? I guess it did, sort of, with Fall. But the doctor would have been wise to apply the cure, if he had it, as quickly as possible to Hepplewhite.<br /><br />There is one moment that was annoying but also necessary. And it is something that appears to recur in these Night Gallery segments. It's Serling's constant need to sermonize. For that's what we got, one more time, with Dr. Fall. I don't know what was more frustrating, losing the black bag and all its miracles or not being to stop Fall from preaching about the bag's benefit for humanity, all while rubbing Hepplewhite's greedy face in the mud, and, therefore, all but begging for Hepplewhite to strike out at him. But as I say, it was necessary. At least it was for me. Otherwise, we wouldn't have been able to see Wills' performance discussed above. All done without moving a muscle or speaking a word.\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,'review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e60c2c50-dec9-4625-a889-8371ced90044"
    }
   },
   "source": [
    "As we can see here, the text contains HTML markup as well as punctuation and other non-letter characters. Since we care only about the semantics, we remove the HTML markup as it does not contain much useful semantics. Also, although punctuation marks might be useful in certain NLP contexts, we remove all punctuation marks for simplicity. One exception is the emoticon characters such as \":)\" since they are certainly useful for sentiment analysis. Furthermore, we convert all text to lowercase since it doesn't matter if reviews are in upper or lower case.\n",
    "\n",
    "In summary, we clean up the text by:\n",
    "* removing all HTML tags;\n",
    "* removing punctuation marks but emoticons;\n",
    "* converting all characters to lowercase.\n",
    "\n",
    "To accomplish this task, we use Python's regular expression (`re`) library, and a powerful HTML parsing tool [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/). If you don't have BeautifulSoup4 yet, you can install it via Anaconda: \n",
    "\n",
    "```\n",
    "> conda install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f8248d14-aee2-47b2-98e3-5c2f081ecfc6"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "edf91a41-9217-473f-9f32-b252eefceaa3"
    }
   },
   "source": [
    "By calling `BeautifulSoup(text, 'html.parser')`, we constructs a BeautifulSoup object, which represents the document as a nested data structure, and you can navigate the tree easily, like selecting a tag or querying tags with some regex pattern (see more on [BeautifulSoup website](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree)). For this example, we simply remove all HTML tags (including the tag properties) and keep only the raw texts between tags by calling the method `get_text()`. After we remove the HTML markup, we used a slightly more complex regex to find emoticons, which we temporarily stored as emoticons. Next we remove all non-word characters from the text via the regex \"[\\W]+\", convert the text into lowercase characters, and add the temporarily stored emoticons to the end of the text. Additionally, we removed the nose character (-) from the emoticons for consistency.\n",
    "\n",
    "Let's do a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5b3f59da-11a9-4423-8973-72f6e63cc372"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is a sanity check  :( ;P\n"
     ]
    }
   ],
   "source": [
    "print(preprocessor('<a href=\"example.com\">Hello, This :-( is a sanity check ;P!</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e3581346-2dab-4c81-acce-63e4221fc993"
    }
   },
   "source": [
    "Our cleaning preprocessor seems to work correctly. That's great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "34203978-cf09-4d2c-a38f-5e746f51f005"
    }
   },
   "source": [
    "Now, we need to think about how to split the text corpora into individual elements. This is called **tokenization**. One way to tokenize documents is to split them into individual words by splitting the cleaned document at its whitespace characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "894c73e5-7cbe-44a7-adc5-9578aeb78b1e"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "print(tokenizer('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4782aa63-3e5b-44e9-bf63-2252e9831dab"
    }
   },
   "source": [
    "The example sentence is now split into tokens. However, we see a problem here: the token \"running\" and \"run\" only differs in verb tense. It is not a good idea to keep them as different tokens as this introduces unnecessary redundancy in the vector representation. Let's merge them using a technique called **word stemming**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6f64e546-66cb-4e06-8c22-6834cfc97b0e"
    }
   },
   "source": [
    "#### Preprocessing: Word Stemming\n",
    "\n",
    "**Word stemming** is a process that transforms words into their root forms and allows us to map related words to the same stem. The original stemming algorithm was developed by Martin F. Porter in 1979 and is hence known as the **Porter stemming** algorithm. The [Natural Language Toolkit for Python](http://www.nltk.org) implements the Porter stemming algorithm, which we use here. In order to install the NLTK, you can simply execute:\n",
    "\n",
    "```\n",
    "> conda install nltk\n",
    "```\n",
    "\n",
    "NOTE: NLTK module provides powerful tools for various NLP tasks, such as the sentiment polarity scoring, common stop words, POS tagging, etc., which you may find useful for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2bc2a077-70cb-4f79-84ee-5ff82556a9f2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenizer_stem(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in re.split('\\s+', text.strip())]\n",
    "\n",
    "print(tokenizer_stem('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b76e3261-bc4b-4cd5-82a1-810c7c0d28b0"
    }
   },
   "source": [
    "As we can see, the word \"running\" is now reduced to its root form \"run\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c343e8dd-6f6e-4e44-a03b-a2895a073d96"
    }
   },
   "source": [
    "NOTE: words stemming just heuristically strips outs prefix or suffix of words. Therefore, it'll produce strange result for some words, ex: the word \"boring\" will be wrongly reduced into non-existing word \"bor\". To overcome this, there's another technique, called **lemmatization**, which **grammatically** transforms words back to root form. Lemmatization is also implemented by NLTK in [nltk.stem](http://www.nltk.org/api/nltk.stem.html). Empirically, there is no much difference in performance between the two techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "93cae5a4-106f-4700-b275-d056d2b69a5d"
    }
   },
   "source": [
    "#### Preprocessing: Stop-Word Removal\n",
    "\n",
    "**Stop-words** are simply words that are extremely common in all sorts of texts thus contain little useful information that can be used to distinguish between different classes of documents. Example stop-words are \"is,\" \"and,\" \"has,\" and \"the.\" Removing stop-words can be useful if we are working with raw or normalized term frequencies such as BoW and Feature Hashing but not for TF-IDF which already downweight frequently occurring words. The BoW, feature hashing, and TF-IDF will be explained in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d18250d8-940e-4041-b3f4-b2afb53c049f"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brandonwu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['runner', 'like', 'run', 'thu', 'run']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n",
    "print(tokenizer_stem_nostop('runners like running and thus they run'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "73045937-bf52-417d-a49e-8701e00a98dd"
    }
   },
   "source": [
    "Since machine learning models only accept numerical features, we must convert categorical features, such as tokens into a numerical form. In the next section, we introduce several commonly used models, including **BoW**, **TF-IDF**, and **Feature Hashing** that allows us to represent text as numerical feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9be71614-f9c1-4374-a518-ad3e82a32468"
    }
   },
   "source": [
    "####  Word2Vec: BoW (Bag-Of-Words)\n",
    "\n",
    "The idea behind bag-of-words model is to represent each document by occurrence of words, which can be summarized as the following steps:\n",
    "\n",
    "1. Build vocabulary dictionary by unique token from the entire set of documents;\n",
    "2. Represent each document by a vector, where each position corresponds to the occurrence of a vocabulary in dictionary.\n",
    "\n",
    "Each vocabulary in BoW can be a single word (1-gram) or a sequence of $n$ continuous words (n-gram). It has been shown empirically that 3-gram or 4-gram BoW models yield good performance in anti-spam email filtering application.\n",
    "\n",
    "Here, we use Scikit-learn's implementation [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to construct the BoW model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9187a679-b190-4c5d-bb5a-3beb1e210a71"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[example documents]\n",
      "Study hard, then you will be happy and I will be happy\n",
      "\"I'm not happy :(\" \", because you don't study hard\n",
      "\n",
      "[vocabulary]\n",
      "{'studi': 2, 'happi': 0, 'hard': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "doc_dummy = [\"Study hard, then you will be happy and I will be happy\", \n",
    "           \"\\\"I'm not happy :(\\\" \\\", because you don't study hard\"]\n",
    "print('[example documents]\\n{}\\n'.format('\\n'.join(doc_dummy)))\n",
    "\n",
    "# ngram_range=(min,max), default: 1-gram => (1,1)\n",
    "count = CountVectorizer(ngram_range=(1, 1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "count.fit(doc_dummy)\n",
    "# dictionary is stored in vocabulary_\n",
    "BoW = count.vocabulary_\n",
    "print('[vocabulary]\\n{}'.format(BoW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "df203c0a-8919-4706-883b-e7f097486a6f"
    }
   },
   "source": [
    "The parameter `ngram_range=(min-length, max-length)` in `CountVectorizer` specifies the vocabulary to be `{min-length}`-gram to `{max-length}`-gram. For example `ngram_range=(1, 2)` will use both 1-gram and 2-gram as vocabularies. After constructing BoW model by calling `fit()`, you can access BoW vocabularies in its attribute `vocubalary_`, which is stored as Python dictionary that maps vocabulary to an integer index.\n",
    "\n",
    "Let's transform the example documents into feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "938c1efa-cb13-43ec-b1e0-c2bb79709259"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(did, vid)\ttf\n",
      "  (0, 0)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "\n",
      "Is document-term matrix a scipy.sparse matrix? True\n"
     ]
    }
   ],
   "source": [
    "# get matrix (doc_id, vocabulary_id) --> tf\n",
    "doc_bag = count.transform(doc_dummy)\n",
    "print('(did, vid)\\ttf')\n",
    "print(doc_bag)\n",
    "\n",
    "print('\\nIs document-term matrix a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bb9a4615-22be-4e3d-9763-280c093c043c"
    }
   },
   "source": [
    "Since each document contains only a small subset of vocabularies, `CountVectorizer.transform()` stores feature vectors as `scipy.sparse` matrix, where entry index is `(document-index, vocabulary-index)` pair, and the value is the **term frequency**---the number of times a vocabulary (term) occurs in a document. For example, `(0,0) 2` means the 1st term \"happy\" appears twice in the 1st document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "46b2089a-2343-4be2-8dce-b5da95cfcedd"
    }
   },
   "source": [
    "Unfortunately, many Scikit-learn classifiers do not support input as sparse matrix now. We can convert `doc_bag` into a Numpy dense matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "a618aada-67ba-4095-810a-1e008f2e0c2a"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1]\n",
      " [1 1 1]]\n",
      "\n",
      "After calling .toarray(), is it a scipy.sparse matrix? False\n"
     ]
    }
   ],
   "source": [
    "doc_bag = doc_bag.toarray()\n",
    "print(doc_bag)\n",
    "\n",
    "print('\\nAfter calling .toarray(), is it a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e6611416-1354-4b67-8685-b9ea82b12f3f"
    }
   },
   "source": [
    "Let's convert part of our movie review into BoW vectors and see what are the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e7776e90-7234-44e9-8912-a9471b99a91b"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "abandon: 230\n",
      "abc: 186\n",
      "abil: 113\n",
      "abl: 94\n",
      "abrupt: 69\n",
      "absenc: 68\n",
      "absolut: 64\n",
      "absorb: 59\n",
      "absurd: 55\n",
      "academ: 55\n"
     ]
    }
   ],
   "source": [
    "doc = df['review'].iloc[:100]\n",
    "doc_bag = count.fit_transform(doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(bag_cnts.argsort()[::-1])[0][:top], \\\n",
    "                        np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fdd13e4e-69e6-4626-82d0-52a42a7c1cb4"
    }
   },
   "source": [
    "To find out most frequent words among documents, we first sum up vocabulary counts in documents, where `axis=0` is the document index. Then, we sort the summed vocabulary count array in ascending order and get the sorted index by `argsort()`. Next, we revert the sorted list by `[::-1]`, and feed into `inverse_transform()` to get corresponding vocabularies. Finally, we show the 20 most frequent vocabularies with their occurrence counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "05043745-3b07-440e-89cb-fbe2f8e259fb"
    }
   },
   "source": [
    "You can observe that some stemmed words like \"abandon\" are not stop-words, but they appear in most documents such that their occurrences become unhelpful to a learning task. Next, we introduce the **TF-IDF** model that **downweights frequently occurring words** among the input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "12fd63af-e58e-42c7-89d9-e879249c265c"
    }
   },
   "source": [
    "#### Word2Vec: TF-IDF (Term-Frequency &amp; Inverse-Document-Frequency)\n",
    "\n",
    "TF-IDF model calculates not only the term-frequency (TF) as BoW model does, but also the **document-frequency** (DF) of a term, which refers to the number of documents that contain this term. The TF-IDF score for a term is defined as\n",
    "\n",
    "$$TF\\text{-}IDF=TF\\cdot\\left(\\log\\left(\\frac{1+N_\\text{doc}}{1+DF}\\right)+1\\right),$$\n",
    "\n",
    "where the $\\log()$ term is called the **inverse-document-frequency** (IDF) and $N_\\text{doc}$ is the total number of documents. The idea behind TF-IDF is to downweight the TF of a word if it appears in many documents. For example, if a word appears in every document, the second term become $\\log(1)+1=1$, which will be smaller than any other word appearing in only a part of documents.\n",
    "\n",
    "NOTE: we add $1$ to both the numerator and denominator inside the $\\log()$ in the above definition so to avoid the numeric issue of dividing by $0$.\n",
    "\n",
    "Let's create the TF-IDF feature representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "fd12f08d-cf3d-48c1-9e3f-40253193343a"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "abandon: 1.31\n",
      "abc: 1.47\n",
      "abil: 1.49\n",
      "abl: 1.66\n",
      "abrupt: 1.85\n",
      "absenc: 1.90\n",
      "absolut: 1.90\n",
      "absorb: 1.90\n",
      "absurd: 1.95\n",
      "academ: 2.00\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "abandon: 7.066901709415235\n",
      "abc: 6.299966884557826\n",
      "abil: 3.4356531496083536\n",
      "abl: 3.4259334935066805\n",
      "abrupt: 3.0908848923824195\n",
      "absenc: 2.9145931936069243\n",
      "absolut: 2.6096711446618777\n",
      "absorb: 2.533831399982588\n",
      "absurd: 2.303352832004743\n",
      "academ: 2.2474055126830565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1),\n",
    "                        preprocessor=preprocessor,\n",
    "                        tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "tfidf.fit(doc)\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[i], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(doc).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(tfidf_sum.argsort()[::-1])[0][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d2d7cd36-2bd0-47c0-9bf1-761036082dff"
    }
   },
   "source": [
    "We can see that the words like \"abandon\" now have downweighted values that are less distant from other words such as \"accept\" not appearing in most documents.\n",
    "\n",
    "Now we have a problem, the number of features that we have created in `doc_tfidf` is huge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3248)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "68c64a89-7b0a-4799-8a36-763ccbb72751"
    }
   },
   "source": [
    "There are more than 3000 features for merely 100 documents. In practice, this may lead to too much memory consumption (even with sparse matrix representation) if we have a large number of vocabularies.\n",
    "\n",
    "#### Word2Vec: Feature Hashing\n",
    "\n",
    "**Feature hashing** reduces the dimension vocabulary space by hashing each vocabulary into a hash table with a fixed number of buckets. As compared to BoW, feature hashing has the following pros and cons:\n",
    "\n",
    "* (+) no need to store vocabulary dictionary in memory anymore\n",
    "* (-) no way to map token index back to token via `inverse_transform()`\n",
    "* (-) no IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "70cd9ee0-8351-4f09-bd64-74986c089211"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[example documents]\n",
      "Study hard, then you will be happy and I will be happy\n",
      "\"I'm not happy :(\" \", because you don't study hard\n",
      "\n",
      "(2, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "print('[example documents]\\n{}\\n'.format('\\n'.join(doc_dummy)))\n",
    "\n",
    "# hash words to 1024 buckets\n",
    "hashvec = HashingVectorizer(n_features=2**10,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "# no .fit needed for HashingVectorizer, since it's defined by the hash function\n",
    "\n",
    "# transform sentences to vectors of dimension 1024\n",
    "doc_hash = hashvec.transform(doc_dummy)\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dec1dadf-4384-472a-af46-10e91e3cb5ff"
    }
   },
   "source": [
    "Ok, now we can transform raw text to feature vectors. Let's do the sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e53710d7-de7f-4ae9-8857-5836dd2a6ea5"
    }
   },
   "source": [
    "#### Sentiment Classification Pipeline\n",
    "\n",
    "Let's use the `LogisticRegression` model to classify the movie reviews into positive and negative classes. As discussed in previous sections, there are several preprocessing steps to do before, so the workflow will be:\n",
    "\n",
    "1. Preprocessing: clean the text, and remove stop words;\n",
    "2. Word2vec: extract feature vectors from the raw review text;\n",
    "3. Classification: train a `LogisticRegression` model to do sentiment classification;\n",
    "4. Evaluate: we'll do 10-fold cross-validation to evaluate general performance.\n",
    "\n",
    "In order to evaluate general performance of our model by 10-fold CV, which trains and evaluates the model 10 times, each on different split of the training and testing sets. It's a tedious task if we repeat steps 1 to 3 for each split ourself, thus we'll use the [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in Scikit-learn to wrap these steps 1 to 3.\n",
    "\n",
    "To emphasize the importance of data preprocessing, we compare the performance of pipelines with/withoud data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "435efc40-bdb2-4689-954f-17f51bca2680"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auc (10-fold cv)]\n",
      "LogisticRegression: 0.881 (+/-0.041)\n",
      "LogisticRegression+(1,2)gram: 0.871 (+/-0.046)\n",
      "LogisticRegression+preprocess: 0.908 (+/-0.031)\n",
      "LogisticRegression+preprocess+hash: 0.856 (+/-0.037)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# randomly sample 1000 examples\n",
    "df_small = df.sample(n=1000, random_state=0)\n",
    "\n",
    "names = ['LogisticRegression', \n",
    "         'LogisticRegression+(1,2)gram',\n",
    "         'LogisticRegression+preprocess',\n",
    "         'LogisticRegression+preprocess+hash']\n",
    "# without preprocessing\n",
    "pipe1 = Pipeline([('vect', CountVectorizer()), \n",
    "                  ('clf', LogisticRegression())])\n",
    "# without preprocessing, use larger ngram range\n",
    "pipe2 = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                  ('clf', LogisticRegression())])\n",
    "# with preprocessing\n",
    "pipe3 = Pipeline([('vect', TfidfVectorizer(preprocessor=preprocessor, \n",
    "                                           tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', LogisticRegression())])\n",
    "# with preprocessing and hasing\n",
    "pipe4 = Pipeline([('vect', HashingVectorizer(n_features=2**10,\n",
    "                                             preprocessor=preprocessor, \n",
    "                                             tokenizer=tokenizer_stem_nostop)), \n",
    "                  ('clf', LogisticRegression())])\n",
    "# CV\n",
    "print('[auc (10-fold cv)]')\n",
    "for name, clf in zip(names, [pipe1, pipe2, pipe3, pipe4]):\n",
    "    scores = cross_val_score(estimator=clf, X=df_small['review'], y=df_small['sentiment'], \\\n",
    "                         cv=10, scoring='roc_auc')\n",
    "    print('%s: %.3f (+/-%.3f)' % (name, scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6c4e7607-e7fd-4b2a-a671-4f31b97d3616"
    }
   },
   "source": [
    "As we can see, the AUC is improved with preprocessing. Furthermore, the feature hashing reduces space consumption at the cost of degraded performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8501609e-efdc-4f1f-81ec-2781d3b32310"
    }
   },
   "source": [
    "#### More Creative Features\n",
    "\n",
    "Now, you can go create your basic set of features for the text in competition. But **don't stop from here**. If you do aware the power of feature engineering, use your creativity to extract more features from the raw text. The more meaningful features you create, the more likely you will get a better score and win.\n",
    "\n",
    "Here are few examples for inspiration:\n",
    "\n",
    "* Weekday on which a news article get published: a news might be more popular if published on weekdays (or weekends);\n",
    "* Channel: sports channel might be more popular than financial channel;\n",
    "* Number of images/links: news might be more attractive if it contains more figures or links;\n",
    "\n",
    "There are lots of other directions you can explore, such as NLP features, length of news, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "69037d80-b32d-4466-9618-9a78b761a3db"
    }
   },
   "source": [
    "## Hint 2: Use Out-of-Core Learning If You Don't Have Enough Memory\n",
    "\n",
    "The size of dataset in the competition (300MB in raw text) is much larger than the example IMDB dataset (80MB in raw text). The dataset, after being represented as feature vectors, may become much larger, and you are unlikely to store all of them in memory. Next, we introduce another training technique called the **Out of Core Learning** to help you train a model using **data streaming**.\n",
    "\n",
    "The idea of Out of Core Learning is similar to the stochastic gradient descent, which updates the model when seeing a minibatch, except that each minibatch is loaded from disk via a data stream. Since we only see a part of the dataset at a time, we can only use the `HashingVectorizer` to transform text into feature vectors because the `HashingVectorizer` does not require knowing the vocabulary space in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fca12889-f15c-43bc-bf59-8526bdc15e57"
    }
   },
   "source": [
    "Let's create a stream to read a chunk of CSV file at a time using the Pandas I/O API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "382219ff-1129-4bf0-87a9-35ec815f57e3"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  I know that Chill Wills usually played lovable...          1\n",
      "1  The arrival of an world famous conductor sets ...          1\n",
      "2  This documentary is such a wonderful example o...          1\n",
      "3  I really tried to like this movie but in the e...          0\n",
      "4  Not one of Monogram's better(not trying to be ...          0\n",
      "5  Don't get me wrong, I assumed this movie would...          0\n",
      "6  The `plot' of this film contains a few holes y...          0\n",
      "7  The best of the seven Sam Fuller movies that I...          1\n",
      "8  A gritty Australian film, with all the element...          1\n",
      "9  There are very few performers today who can ke...          1\n"
     ]
    }
   ],
   "source": [
    "def get_stream(path, size):\n",
    "    for chunk in pd.read_csv(path, chunksize=size):\n",
    "        yield chunk\n",
    "\n",
    "print(next(get_stream(path='datasets/train.csv', size=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8ca1a37b-f968-4a1b-a85f-d1052a2148f4"
    }
   },
   "source": [
    "Good. Our stream works correctly. \n",
    "\n",
    "For out-of core learning, we have to use models that can train and update the model's weight iteratively. Here, we use the `SGDClassifier` to train a `LogisticRegressor` using the stochastic gradient descent. We can partial update `SGDClassifier` by calling the `partial_fit()` method. Our workflow now becomes:\n",
    "\n",
    "1. Stream documents directly from disk to get a mini-batch (chunk) of documents;\n",
    "2. Preprocess: clean and remove stop-words in the mini-batch of documents;\n",
    "3. Word2vec: use `HashingVectorizer` to extract features from text;\n",
    "4. Update `SGDClassifier` and go back to step 1.\n",
    "\n",
    "Let's do the out-of core learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8e276288-179c-4c45-adb7-c198f5d9ed9c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000/25000] 0.8785155639919747\n",
      "[4000/25000] 0.8990255376344086\n",
      "[6000/25000] 0.9116987753492246\n",
      "[8000/25000] 0.9145485412929094\n",
      "[10000/25000] 0.918076923076923\n",
      "[12000/25000] 0.9234827757244116\n",
      "[14000/25000] 0.9423918269230769\n",
      "[16000/25000] 0.943243772975092\n",
      "[18000/25000] 0.9394590313445015\n",
      "[20000/25000] 0.9318567426970787\n",
      "[22000/25000] 0.9413999935934397\n",
      "[24000/25000] 0.9462890625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**20, \n",
    "                            preprocessor=preprocessor, tokenizer=tokenizer_stem_nostop)\n",
    "# loss='log' gives logistic regression\n",
    "clf = SGDClassifier(loss='log', n_iter=100)\n",
    "batch_size = 1000\n",
    "stream = get_stream(path='datasets/train.csv', size=batch_size)\n",
    "classes = np.array([0, 1])\n",
    "train_auc, val_auc = [], []\n",
    "# we use one batch for training and another for validation in each iteration\n",
    "iters = int((25000+batch_size-1)/(batch_size*2))\n",
    "for i in range(iters):\n",
    "    batch = next(stream)\n",
    "    X_train, y_train = batch['review'], batch['sentiment']\n",
    "    if X_train is None:\n",
    "        break\n",
    "    X_train = hashvec.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    train_auc.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))\n",
    "    \n",
    "    # validate\n",
    "    batch = next(stream)\n",
    "    X_val, y_val = batch['review'], batch['sentiment']\n",
    "    score = roc_auc_score(y_val, clf.predict_proba(hashvec.transform(X_val))[:,1])\n",
    "    val_auc.append(score)\n",
    "    print('[{}/{}] {}'.format((i+1)*(batch_size*2), 25000, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3dcc4db5-6a09-4a25-aabb-e2fd712fc7d1"
    }
   },
   "source": [
    "After fitting `SGDClassifier` by an entire pass over training set, let's plot the learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ee5b5c2e-bb19-4409-92fb-eecde37a1ff5"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkdhSSImISGwppEREJLb+H6SZRu3IEB1AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12167d710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(train_auc)+1), train_auc, color='blue', label='Train auc')\n",
    "plt.plot(range(1, len(train_auc)+1), val_auc, color='red', label='Val auc')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('#Batches')\n",
    "plt.ylabel('Auc')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-out-of-core.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fcce1c30-7db8-42c8-8d20-48c915782af7"
    }
   },
   "source": [
    "The learning curve looks great! The validation accuracy improves as more examples are seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dcedf9b5-a3fd-47d8-a2c9-5e462d56f4dd"
    }
   },
   "source": [
    "Since training `SGDClassifier` may take long, you can save your trained classifier to disk periodically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "3166d15b-039d-4a0a-9ca3-08248770432f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 0.930\n"
     ]
    }
   ],
   "source": [
    "# import optimized pickle written in C for serializing and \n",
    "# de-serializing a Python object\n",
    "import _pickle as pkl\n",
    "\n",
    "# dump to disk\n",
    "pkl.dump(hashvec, open('output/hashvec.pkl', 'wb'))\n",
    "pkl.dump(clf, open('output/clf-sgd.pkl', 'wb'))\n",
    "\n",
    "# load from disk\n",
    "hashvec = pkl.load(open('output/hashvec.pkl', 'rb'))\n",
    "clf = pkl.load(open('output/clf-sgd.pkl', 'rb'))\n",
    "\n",
    "df_test = pd.read_csv('datasets/test.csv')\n",
    "print('test auc: %.3f' % roc_auc_score(df_test['sentiment'], \\\n",
    "            clf.predict_proba(hashvec.transform(df_test['review']))[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ee02e25f-ed16-45a7-95ed-2d983f592c67"
    }
   },
   "source": [
    "Now you have the all the supporting knowledge for the competition. Happy coding and good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ab05cae8-8927-4eeb-8bd1-d892269fd28d"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
